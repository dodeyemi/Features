# -*- coding: utf-8 -*-
"""Feature Engineering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1csXDPMFo8EJfDSu6oVytpID1BnpKZiTL
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from skimage.feature import hog
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

dataa = pd.read_csv('Bio_Data (2).csv')

dataa.dtypes

dataa.describe()

Groupby_Data = dataa.groupby('repaymentStatus')['amount'].sum().reset_index()

sns.barplot(x = 'repaymentStatus', y = 'amount', data = Groupby_Data)
plt.title('Class Distribution')
plt.xlabel('ReapymentStatus')
plt.ylabel('Total_Disbursed')

Completed_Loans = dataa[dataa['repaymentStatus']==1]
Completed_Loans['repaymentStatus'].count()

Active_Loans = dataa[dataa['repaymentStatus']==0]
Active_Loans['repaymentStatus'].count()

Class = dataa['repaymentStatus']
Class

Features = dataa.drop('repaymentStatus', axis = 1)
Features.tail(3)

"""##**Feauture Selections- F_Regressions Technics**##"""

from sklearn.feature_selection import SelectKBest, f_regression, chi2


# Apply the KBest feature selection
k = 3  # Number of best features to select
kbest = SelectKBest(score_func=f_regression, k=k)
selected_features = kbest.fit_transform(Features, Class)

# Get the indices of the selected features
selected_indices = kbest.get_support(indices=True)

# Get the names of the selected features
selected_feature_names = Features.columns[selected_indices]

# Print the selected feature names
print("Selected Features:")
for feature in selected_feature_names:
    print(feature)



"""##**Feature Selection With Chi2 Technic**##"""

from sklearn.preprocessing import MinMaxScaler



# Apply the MinMaxScaler to scale the features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(Features)

# Apply the KBest feature selection
k = 3  # Number of best features to select
kbest = SelectKBest(score_func=chi2, k=k)
selected_features = kbest.fit_transform(X_scaled, Class)

# Get the indices of the selected features
selected_indices = kbest.get_support(indices=True)

# Get the names of the selected features
selected_feature_names = Features.columns[selected_indices]

# Print the selected feature names
print("Selected Features:")
for feature in selected_feature_names:
    print(feature)

"""##**Feature Selection with Xtra Tree Classifier**##"""

import matplotlib.pyplot as plt
from sklearn.ensemble import ExtraTreesClassifier

# Feature selection with ExtraTreeClassifier
model = ExtraTreesClassifier()
model.fit(Features, Class)

# Get feature importances
importances = model.feature_importances_

# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]

# Select the top 10 features
k = 20  # Number of best features to select
selected_indices = indices[:k]
selected_features = Features.columns[selected_indices]
selected_importances = importances[selected_indices]

# Plot bar chart
plt.figure(figsize=(10, 6))
plt.bar(selected_features, selected_importances)
plt.xticks(rotation=65)
plt.xlabel("Features")
plt.ylabel("Feature Importance")
plt.title("Top 20 Features - ExtraTreeClassifier Model")
plt.tight_layout()
plt.show()

"""##Comments##

The above Tree Classifier above shows the feature according to their importance in bar chart. The tallest bar shows how the features  are important when looking at the target variable Class.

##**Heatmap or Correlation Technic**##

This is useful to show the variable with high correlation with the target variable, the repaymentStatus. It looks like almost all the technic of feature engineering technic agrees with one another except in few cases.
"""

Correlations = dataa.corr()
Correlations

"""##Visualisation of the Correlation Technics of Feature Selections##"""

sns.set(rc={'figure.figsize': (20, 8)})

# Plot heatmap
sns.heatmap(Correlations, xticklabels=Correlations.columns, yticklabels=Correlations.columns, annot=True)

# Display the plot
plt.show()

"""##**Feature Engineering in Images Processing**##

In this session, we will be using different methods of feature extractions to form a pandas data frame and use feature engineering technic to select the best for machine learning model. It means we will be using images like subsea data images.
"""

from google.colab import drive # The code reading the folder containing all the images in the zip folders on google drive

drive.mount("/content/gdrive")

!pip install opencv-python
!pip install keras 
!pip install tensorflow

# Commented out IPython magic to ensure Python compatibility.
import cv2 
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import glob
import os # Inporting all the relevant libraries that will be needed to read the multiple images from the surface datasets of images provided in the coursework
import numpy as np #Loading the numpy array library to allow for basic arithmetic and data some preparation functions
from sklearn.model_selection import train_test_split #
from sklearn import svm
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score ## Running the Scores Library from the SVM
import numpy as np #Importing the numpy library. Its installation mostly comes with open-cv. It can also be installed seperatly.
from keras.datasets import mnist
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
#from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf
from skimage.feature import corner_harris, corner_peaks
from skimage.morphology import dilation, square
import pandas as pd

resize_width = 100 #Resizing the width of the images to 100
resize_height = 100 #Resizing the Lenghts of the Images to 100
path = '/content/gdrive/MyDrive/Dataset/Surface/' #Reading the path
_images = [] # List to append the images as 2D numpy arrays.
_target = [] # List to append the target
O_repo = [] # Create a repo for flattened pixels
repothreshold1 = [] #Creeating the empty list for  Binarised images for feature extraction

for root, dirs, files in os.walk(path): #Looping through each images through the directories in the gdrive path.
     for file in files:
        with open(os.path.join(root, file), "r") as auto:
            try:
                img = cv2.imread(root+'/'+file, 0) #Reading the images into a variable img
                #imgshow = plt.imshow(img)
                img = cv2.resize(img, (resize_width, resize_height)) #Effecting the resized template.
                ret,threshold1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY) #This is not required at this level. It was just created if there is need for further research on this work.
                repothreshold1.append(threshold1)
                _images.append(img)
                # Append the flattened image to the pixel repo
                O_repo.append(img.flatten())
                # Append the folder where the image is to the target list
                _target.append(root.replace(path,'').replace('\\','').replace('/','')) #joining the target in the path directory to the variables and replacing slashes with nothing.
            except Exception as e:
                print("Invalid file "+file+" skipped.")
# Convert the repo list into numpy array
O_repo = np.array(O_repo)
repothreshold1 = np.array(repothreshold1)
_images = np.array(_images)
#_target = np.array(_target)
#plt.imshow(_images)
print('Threshold Datasets:', repothreshold1)
print('The classes of Surface data:', _target)
print('Shapes of the surface data:',repothreshold1.shape)
print('The size of the Surface datasets:', repothreshold1.size)
print('The shape of the surfaces', _images.shape)
#print('The size of the Target Images:', _target.size)

# Define the number of rows and columns for the subplot grid
nrows = 2 # Number of columns
ncols = 3 # Number of rows.

# Create a new figure and set its size
fig, axs = plt.subplots(nrows, ncols, figsize=(15, 10))

# Loop through the images and plot them in the subplot grid
for i in range(nrows):
  for j in range(ncols):
    # Compute the index of the current image
    idx = i * ncols + j
    # Get the image from the repothreshold4 list
    img = _images[idx]
    #img = repothreshold1[idx]
    # Plot the image in the subplot grid
    axs[i, j].imshow(img, cmap='gray')
    axs[i, j].axis('off')

# Show the plot
plt.show()

# Count the occurrences of each target value
target_counts = {}
for target in _target:
    if target in target_counts:
        target_counts[target] += 1
    else:
        target_counts[target] = 1

# Create a bar chart of the target counts
plt.bar(target_counts.keys(), target_counts.values())

# Add axis labels and title
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Distribution of Surface Classes')

# Show the plot
plt.show()

target_Binarisation = [] #Create an empty list for binarised target.
target_Binarisation = [1 if i == 'Positive' else 0 for i in _target] #Ascribe number 1 to positive and 0 to negative since this is not a medical data.
target_Binarisation=np.array(target_Binarisation) #Convert the target to numpy array
print(target_Binarisation,target_Binarisation.shape) #Print the shape of the new class.

"""##Harris Corners Method##"""

harris_corners = []
for img in _images:
    # Convert the image to grayscale if necessary
    img_gray = img  # Replace this line with your grayscale conversion code if needed

    # Apply Harris corner detection
    corners = corner_peaks(corner_harris(img_gray), min_distance=5)

    # Add the corner coordinates to the list
    harris_corners.append(corners)

# Step 3: Convert the list of corner coordinates to a feature matrix
corner_features = np.array(harris_corners)

harris_corners[0]

harris_corners = []
for img in _images:
    # Convert the image to grayscale if necessary
    img_gray = img  # Replace this line with your grayscale conversion code if needed

    # Apply Harris corner detection
    corners = corner_peaks(corner_harris(img_gray), min_distance=5)

    # Add the corner coordinates to the list
    harris_corners.append(corners)

# Step 3: Dilate the Harris corners
dilated_corners = [dilation(corners, selem=square(3)) for corners in harris_corners]

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))
axes[0, 0].imshow(_images[0], cmap='gray')
axes[0, 0].set_title('Original Image')
axes[0, 1].imshow(dilated_corners[0], cmap='gray')
axes[0, 1].set_title('Dilated Image')
axes[1, 0].imshow(_images[1], cmap='gray')
axes[1, 0].set_title('Original Image')
axes[1, 1].imshow(dilated_corners[1], cmap='gray')
axes[1, 1].set_title('Dilated Image')

plt.tight_layout()
plt.show()



# Step 1: Flatten the list of dilated corners
flattened_corners = np.concatenate(dilated_corners).reshape(-1, 1)

# Step 2: Create a DataFrame with one column
df = pd.DataFrame(flattened_corners, columns=['Dilated Corners'])


#df['Target'] =target_values

# Step 3: Print the DataFrame
print(df)

df['Target'] = pd.DataFrame(_target, columns=['Class'])
df

"""##HOG Method##"""

def extract_hog_features(images):
    hog_features = []
    for img in _images:
        # Convert the image to grayscale if necessary
        img_gray = img  # Replace this line with your grayscale conversion code if needed

        # Extract HOG features
        features = hog(img_gray, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2))

        # Add the features to the list
        hog_features.append(features)

    return hog_features

# Step 1: Extract HOG features from the images
hog_features = extract_hog_features(_images)

# Step 2: Convert the list of HOG features to a feature matrix
hog_features = np.array(hog_features)
hog_features.shape

"""##This is just test, the intention here is not to run a model but to do feature extraction##"""

X_train, X_test, y_train, y_test = train_test_split(hog_features, _target, test_size=0.2, random_state=42)

# Step 4: Train a classifier (SVM in this example) using the HOG features
classifier = SVC()
classifier.fit(X_train, y_train)

# Step 5: Predict the classes for the test set
predictions = classifier.predict(X_test)

# Step 6: Calculate accuracy
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)

"""##Thresholding Method##"""

def extract_thresholding_features(images):
    threshold_features = []
    for img in _images:
        # Convert the image to grayscale if necessary
        img_gray = img  # Replace this line with your grayscale conversion code if needed

        # Apply thresholding
        threshold_value = threshold_otsu(img_gray)
        binary_image = (img_gray > threshold_value).astype(int)  # Convert boolean to integer

        # Flatten the binary image and add it to the features list
        threshold_features.append(binary_image.flatten())

    return threshold_features

# Step 1: Extract thresholding features from the images
threshold_features = extract_thresholding_features(_images)

# Step 2: Convert the list of thresholding features to a feature matrix
threshold_features = np.array(threshold_features)
threshold_features.shape